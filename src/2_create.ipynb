{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating HF dataset\n",
    "This NB reads the exising data and creates a HF dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk, concatenate_datasets, DatasetInfo, DatasetDict\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images\n",
    "def load_image(image_path):\n",
    "    return Image.open(image_path)\n",
    "\n",
    "# Directory containing images and pandas dataframes\n",
    "output_folder = \"../data/Chinese\"\n",
    "text_dir = f\"{output_folder}/texts\"\n",
    "image_dir = f\"{output_folder}/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each dataframe and create a Hugging Face dataset\n",
    "def process_dataframe(df):\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    # Function to map the image loading for each row\n",
    "    def process_images(example):\n",
    "        image_filename = example['identifier'] + '.png' \n",
    "        image_path = os.path.join(image_dir, image_filename)  # 'identifier' is the image file name\n",
    "        example['image'] = load_image(image_path)\n",
    "        return example\n",
    "\n",
    "    # Apply the image loading function\n",
    "    dataset = dataset.map(process_images)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [pd.read_csv(x) for x in glob.glob(f\"{text_dir}/*.csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4/4 [00:00<00:00, 392.70 examples/s]\n",
      "Map: 100%|██████████| 7/7 [00:00<00:00, 1851.09 examples/s]\n",
      "Map: 100%|██████████| 7/7 [00:00<00:00, 1941.17 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 1389.30 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 1026.42 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1542.82 examples/s]\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 2513.73 examples/s]\n",
      "Map: 100%|██████████| 7/7 [00:00<00:00, 2143.23 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 2768.21 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 1035.03 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 2169.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Combine datasets from each dataframe\n",
    "datasets = [process_dataframe(df) for df in dataframes]\n",
    "combined_dataset = concatenate_datasets(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = DatasetInfo(\n",
    "    description=\"This dataset includes images with corresponding text captions for OCR tests in Jawi (Malay written with Arabic letters). The data is taken from a publicly available repository of the Warta Malaya newspaper.\",\n",
    "    citation=\"\"\"\n",
    "    @dataset{miguelescobarvarela_jawi_ocr_2024,\n",
    "      author    = {NUS},\n",
    "      title     = {Gongguan-OCR-1},\n",
    "      year      = {2025},\n",
    "      version   = {1.0.0},\n",
    "      publisher = {Hugging Face},\n",
    "      url       = {https://huggingface.co/datasets/your-username/Jawi-OCR-1}\n",
    "    }\n",
    "    \"\"\",\n",
    "    license=\"CC BY-SA 4.0\",  # Example license\n",
    "    version=\"1.0.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_dataset.info = dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_dataset.save_to_disk('../../data/hf/Jawi-OCR-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting into train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset (80% train, 20% validation)\n",
    "train_test_split = combined_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to specify specific columns to split, use the split function like this:\n",
    "# train_test_split = dataset['your_dataset_name'].train_test_split(test_size=0.2)\n",
    "\n",
    "# Create a DatasetDict for saving\n",
    "split_dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': train_test_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to HF hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you add tokens/ to .gitignore to avoid sharing your tokens publicly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokens/token\") as f:\n",
    "    token = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A:   0%|          | 0/55 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 55/55 [00:00<00:00, 483.93 examples/s]\n",
      "\n",
      "\u001b[Aating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  4.99ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.29s/it]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 1342.33 examples/s]\n",
      "\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 29.69ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/mevsg/Gongguan-OCR-v1/commit/a63da6cd8c81444b4d995aa6a2cb0b15d6b5ab8e', commit_message='Upload dataset', commit_description='', oid='a63da6cd8c81444b4d995aa6a2cb0b15d6b5ab8e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/mevsg/Gongguan-OCR-v1', endpoint='https://huggingface.co', repo_type='dataset', repo_id='mevsg/Gongguan-OCR-v1'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset.push_to_hub(\"mevsg/Gongguan-OCR-v1\", token=token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
