{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text and images (Classical Chinese)\n",
    "\n",
    "This file identifies the text lines manually labelled in Transkribus. It saves individual lines as text and images to different files, which can then be used to create a HF dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "import glob\n",
    "import itertools\n",
    "import unicodedata\n",
    "import os \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\alexm\\NUS Dropbox\\Alexander Mozdzen\\ocr\\gongguan-ocr-1\\src\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(f\"Current working directory: {cwd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"../data\"\n",
    "input_folders = [\n",
    "    f\"{output_folder}/01_11\",\n",
    "    f\"{output_folder}/12_21\", \n",
    "    f\"{output_folder}/22_31\",\n",
    "    f\"{output_folder}/32_41\"\n",
    "]\n",
    "# create output folders\n",
    "os.makedirs(f\"{output_folder}/texts\", exist_ok=True)\n",
    "os.makedirs(f\"{output_folder}/images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_chinese_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text to standard Chinese Unicode form.\n",
    "    Converts variant Unicode characters (e.g., Kangxi Radicals) into normal forms.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize(\"NFKC\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New version that uses the included index in the XML file for the reading order of the text lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml_index_ordering(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    namespace = {\n",
    "        'ns': 'http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15'\n",
    "    }\n",
    "\n",
    "    # initialize a dict of empty lists\n",
    "    regions = {\n",
    "        \"region_id\": [],\n",
    "        \"coord_str\": [],\n",
    "        \"ordered_text\": []\n",
    "    }\n",
    "\n",
    "    # iterate through each TextRegion\n",
    "    for text_region in root.findall(\".//ns:TextRegion\", namespace):\n",
    "        coords_elem = text_region.find(\"ns:Coords\", namespace)\n",
    "        if coords_elem is None:\n",
    "            continue\n",
    "\n",
    "        # pull out the region ID & its coords\n",
    "        region_id = text_region.get(\"id\")\n",
    "        coords_str = coords_elem.get(\"points\")\n",
    "\n",
    "        text_lines = []\n",
    "        for line in text_region.findall(\".//ns:TextLine\", namespace):\n",
    "            unicode_elem = line.find(\".//ns:Unicode\", namespace)\n",
    "            if unicode_elem is None:\n",
    "                continue\n",
    "            try:\n",
    "                normalized = normalize_chinese_text(unicode_elem.text) if unicode_elem.text else \"\"\n",
    "            except Exception:\n",
    "                normalized = \"\"\n",
    "            try:\n",
    "                index = int(line.get(\"custom\").split(\"{index:\")[1].split(\";}\")[0])\n",
    "            except Exception:\n",
    "                continue  # skip if index is malformed\n",
    "            text_lines.append((normalized, index))\n",
    "        \n",
    "        # sort by the index and join into one string\n",
    "        text_lines.sort(key=lambda x: x[1])\n",
    "        ordered_text = \" \".join(text for text, _ in text_lines)\n",
    "\n",
    "        # append into each list in our dict\n",
    "        regions[\"region_id\"].append(region_id)\n",
    "        regions[\"coord_str\"].append(coords_str)\n",
    "        regions[\"ordered_text\"].append(ordered_text)\n",
    "\n",
    "    return regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_images_from_regions_subfolders(subfolder_name, page_name, image_path, regions,\n",
    "                                          buffer_above=10, buffer_below=10,\n",
    "                                          buffer_left=10, buffer_right=10):\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    for region_id, coords_str in zip(regions[\"region_id\"], regions[\"coord_str\"]):\n",
    "        points = [tuple(map(int, point.split(','))) for point in coords_str.split()]\n",
    "        x_coords, y_coords = zip(*points)\n",
    "\n",
    "        min_x = min(x_coords) - buffer_left\n",
    "        max_x = max(x_coords) + buffer_right\n",
    "        min_y = min(y_coords) - buffer_above\n",
    "        max_y = max(y_coords) + buffer_below\n",
    "\n",
    "        cropped_image = image.crop((min_x, min_y, max_x, max_y))\n",
    "        cropped_image.save(f\"{output_folder}/images/{subfolder_name}_{page_name}_{region_id}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: iterdir + is_dir\n",
    "\n",
    "output_folder = Path(\"../data/\")\n",
    "for subfolder in output_folder.iterdir():\n",
    "    image_paths = list(subfolder.glob(\"*.jpg\"))\n",
    "    filenames = [p.stem for p in image_paths]\n",
    "    xml_paths = glob.glob(f\"{subfolder}/page/*.xml\")\n",
    "    for page_name, xml_path, image_path in zip(filenames, xml_paths, image_paths):\n",
    "    \n",
    "        regions = parse_xml_index_ordering(xml_path)\n",
    "        # Save text information as dataframe:\n",
    "        pd.DataFrame({\n",
    "        \"text\": regions[\"ordered_text\"],\n",
    "        \"identifier\": [f\"{subfolder.name}_{page_name}_{region_id}\" for region_id in regions[\"region_id\"]]\n",
    "        }).to_csv(f\"{output_folder}/texts/{subfolder.name}_{page_name}.csv\", index=False)\n",
    "        \n",
    "         # Create images from the text regions\n",
    "        create_images_from_regions_subfolders(subfolder.name, page_name, image_path, regions,\n",
    "                                                buffer_above=0, buffer_below=0, buffer_left=0, buffer_right=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
