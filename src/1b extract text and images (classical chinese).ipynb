{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text and images (Classical Chinese)\n",
    "\n",
    "This file identifies the text lines manually labelled in Transkribus. It saves individual lines as text and images to different files, which can then be used to create a HF dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "import glob\n",
    "import itertools\n",
    "import unicodedata\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dirs\n",
    "output_folder = \"../data/Chinese\"\n",
    "input_folder = f\"{output_folder}/7030191/Gongguan_sample\"\n",
    "text_output_folder = f\"{output_folder}/texts\"\n",
    "image_output_folder = f\"{output_folder}/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_chinese_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text to standard Chinese Unicode form.\n",
    "    Converts variant Unicode characters (e.g., Kangxi Radicals) into normal forms.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize(\"NFKC\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_vertical_text(text_lines):\n",
    "    \"\"\"\n",
    "    Reorder Chinese text from row-major to column-major for vertical text.\n",
    "    \"\"\"\n",
    "    # Determine max length (widest line)\n",
    "    max_length = max(len(line) for line in text_lines)\n",
    "\n",
    "    # Pad all lines to the same length to ensure proper alignment\n",
    "    padded_lines = [line.ljust(max_length, \" \") for line in text_lines]\n",
    "\n",
    "    # Transpose: Convert rows to columns (read column-by-column, top-to-bottom)\n",
    "    vertical_text = [\"\".join(column) for column in itertools.zip_longest(*padded_lines, fillvalue=\" \")]\n",
    "\n",
    "    return \"\".join(vertical_text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse XML\n",
    "def parse_xml(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    namespace = {'ns': 'http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15'}\n",
    "\n",
    "    regions = []\n",
    "\n",
    "    # Iterate over TextRegion elements\n",
    "    for text_region in root.findall(\".//ns:TextRegion\", namespace):\n",
    "        region_id = text_region.get(\"id\")\n",
    "        coords_elem = text_region.find(\"ns:Coords\", namespace)\n",
    "\n",
    "        if coords_elem is None:\n",
    "            continue\n",
    "\n",
    "        # Extract individual TextLines (each one is a vertical string)\n",
    "        text_lines = [\n",
    "            line.find(\".//ns:Unicode\", namespace).text\n",
    "            for line in text_region.findall(\".//ns:TextLine\", namespace)\n",
    "            if line.find(\".//ns:Unicode\", namespace) is not None\n",
    "        ]\n",
    "\n",
    "        if not text_lines:\n",
    "            continue  # Skip empty regions\n",
    "\n",
    "        # Normalize all extracted text\n",
    "        normalized_lines = [normalize_chinese_text(line) for line in text_lines]\n",
    "\n",
    "        # Convert text_lines into vertical text format\n",
    "        vertical_text = reorder_vertical_text(normalized_lines)\n",
    "\n",
    "        # Extract coordinates\n",
    "        coords_str = coords_elem.get(\"points\")\n",
    "        regions.append((region_id, coords_str, vertical_text))\n",
    "\n",
    "    return regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse XML with index ordering\n",
    "def parse_xml_index_ordering(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    namespace = {'ns': 'http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15'}\n",
    "\n",
    "    regions = []\n",
    "\n",
    "    # Iterate over TextRegion elements\n",
    "    for text_region in root.findall(\".//ns:TextRegion\", namespace):\n",
    "        region_id = text_region.get(\"id\")\n",
    "        coords_elem = text_region.find(\"ns:Coords\", namespace)\n",
    "\n",
    "        if coords_elem is None:\n",
    "            continue\n",
    "\n",
    "        # Extract TextLines with their indices\n",
    "        text_lines_with_indices = []\n",
    "        for line in text_region.findall(\".//ns:TextLine\", namespace):\n",
    "            # Get the index from the custom attribute\n",
    "            custom_attr = line.get(\"custom\", \"\")\n",
    "            index = 0  # Default index if not found\n",
    "            \n",
    "            # Parse the index from the custom attribute\n",
    "            if \"readingOrder {index:\" in custom_attr:\n",
    "                try:\n",
    "                    index_str = custom_attr.split(\"readingOrder {index:\")[1].split(\"}\")[0]\n",
    "                    index = int(index_str)\n",
    "                except (IndexError, ValueError):\n",
    "                    pass  # Keep default index if parsing fails\n",
    "            \n",
    "            # Get the text content\n",
    "            unicode_elem = line.find(\".//ns:Unicode\", namespace)\n",
    "            if unicode_elem is not None and unicode_elem.text:\n",
    "                text = unicode_elem.text\n",
    "                # Normalize the text\n",
    "                normalized_text = normalize_chinese_text(text)\n",
    "                text_lines_with_indices.append((index, normalized_text))\n",
    "        \n",
    "        if not text_lines_with_indices:\n",
    "            continue  # Skip empty regions\n",
    "        \n",
    "        # Sort text lines by index\n",
    "        text_lines_with_indices.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Extract just the text in order\n",
    "        ordered_text = \" \".join(text for _, text in text_lines_with_indices)\n",
    "        \n",
    "        # Extract coordinates\n",
    "        coords_str = coords_elem.get(\"points\")\n",
    "        regions.append((region_id, coords_str, ordered_text))\n",
    "\n",
    "    return regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw each text line with buffer and save as image\n",
    "def create_images_from_regions(page_name, image_path, regions, buffer_above=10, buffer_below=10, buffer_left=10, buffer_right=10):\n",
    "    data = []\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    for idx, (region_id, coords_str, region_text) in enumerate(regions):\n",
    "        # Parse the coordinates and find the bounding box\n",
    "        points = [tuple(map(int, point.split(','))) for point in coords_str.split()]\n",
    "        \n",
    "        x_coords = [p[0] for p in points]\n",
    "        y_coords = [p[1] for p in points]\n",
    "\n",
    "        min_x, max_x = min(x_coords) - buffer_left, max(x_coords) + buffer_right\n",
    "        min_y, max_y = min(y_coords) - buffer_above, max(y_coords) + buffer_below\n",
    "\n",
    "        # Crop the image to the bounding box\n",
    "        cropped_image = image.crop((min_x, min_y, max_x, max_y))\n",
    "\n",
    "        # Append the identifier and text to the data list\n",
    "        data.append([region_text, f'{page_name}_{region_id}'])\n",
    "\n",
    "        # Save text data to a CSV file\n",
    "        df = pd.DataFrame(data, columns=['text', 'identifier'])\n",
    "        df.to_csv(f'{text_output_folder}/{page_name}.csv', index=False)\n",
    "\n",
    "        # Save the cropped image\n",
    "        cropped_image.save(f'{image_output_folder}/{page_name}_{region_id}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of parse_xml() results:\n",
      "--------------------------------------------------\n",
      "Processing file: ../data/Chinese/7030191/Gongguan_sample/page/0001_p001.xml\n",
      "\n",
      "Text lines for region r_96:\n",
      "Index 0: 王蹇觀叫\n",
      "Index 1: 陳情觀叫\n",
      "Index 2: 羅章觀 \n",
      "Index 3: 陳天雨\n",
      "Index 4: 和息\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example: Parse a single XML file and display the results\n",
    "print(\"Example of parse_xml() results:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Get the first XML file from the input folder\n",
    "xml_files = glob.glob(f\"{input_folder}/page/*.xml\")\n",
    "if xml_files:\n",
    "    # Normalize path and convert backslashes to forward slashes\n",
    "    example_xml_path = os.path.normpath(xml_files[0]).replace(\"\\\\\", \"/\")\n",
    "    print(f\"Processing file: {example_xml_path}\")\n",
    "    \n",
    "    # Parse the XML file\n",
    "    # regions = parse_xml(example_xml_path)\n",
    "    tree = ET.parse(example_xml_path)\n",
    "    root = tree.getroot()\n",
    "    namespace = {'ns': 'http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15'}\n",
    "\n",
    "    regions = []\n",
    "\n",
    "    # Iterate over TextRegion elements\n",
    "    for text_region in root.findall(\".//ns:TextRegion\", namespace):\n",
    "        region_id = text_region.get(\"id\")\n",
    "        coords_elem = text_region.find(\"ns:Coords\", namespace)\n",
    "\n",
    "        if coords_elem is None:\n",
    "            continue\n",
    "\n",
    "        # Extract individual TextLines (each one is a vertical string)\n",
    "        # Each tuple contains (text, index) for each line\n",
    "        text_lines = [\n",
    "            (line.find(\".//ns:Unicode\", namespace).text,  # The text content\n",
    "             int(line.get(\"custom\").split(\"{index:\")[1].split(\";}\")[0]))  # The index number\n",
    "            for line in text_region.findall(\".//ns:TextLine\", namespace)\n",
    "            if line.find(\".//ns:Unicode\", namespace) is not None\n",
    "        ]\n",
    "        # text_lines will be a list of tuples, where each tuple is:\n",
    "        # text_lines[i][0] = text content\n",
    "        # text_lines[i][1] = index number\n",
    "        \n",
    "        # Sort text lines by index and return just the text and index tuples\n",
    "        # Sort text_lines list by the index number (x[1]) of each tuple\n",
    "        # text_lines contains tuples of (text_content, index)\n",
    "        # lambda x: x[1] tells sorted() to use the index (second element) as the sorting key\n",
    "        sorted_lines = sorted(text_lines, key=lambda x: x[1])\n",
    "        \n",
    "        # Print out text lines and indices for first region\n",
    "        if len(regions) == 0:  # Only print for first region\n",
    "            print(f\"\\nText lines for region {region_id}:\")\n",
    "            for text, idx in sorted_lines:\n",
    "                print(f\"Index {idx}: {text}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "        regions.append((region_id, sorted_lines))\n",
    "\n",
    "        # for i in enumerate(text_lines):\n",
    "        #     print(f\"\\ntext_lines {i+1}:\")\n",
    "        #     print(f\"index: {text_lines[i][1]}\")\n",
    "        #     print(f\"text content: {text_lines[i][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following code extracts the coordinates of the text regions from the xml files and crops them into individual images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xml_file_path in glob.glob(f\"{input_folder}/page/*.xml\"):\n",
    "    # Normalize path and convert backslashes to forward slashes\n",
    "    xml_file_path = os.path.normpath(xml_file_path).replace(\"\\\\\", \"/\")\n",
    "    page_name = xml_file_path.split(\"/\")[-1].replace(\".xml\", \"\")\n",
    "    image_file_path = f\"{input_folder}/{page_name}.jpg\" \n",
    "\n",
    "    # Parse XML and extract text regions\n",
    "    regions = parse_xml(xml_file_path)\n",
    "\n",
    "    # Create images from the text regions\n",
    "    create_images_from_regions(page_name, image_file_path, regions, buffer_above=0, buffer_below=0, buffer_left=0, buffer_right=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we got an error because glob.glob uses the backslashes of windows and returned:\n",
    "'../data/Chinese/7030191/Gongguan_sample/page\\\\0001_p001.xml'\n",
    "Fixing the backerror by normalizing the path using os.path.normpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/Chinese/7030191/Gongguan_sample/page\\\\0001_p001.xml'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(next(glob.glob(f\"{input_folder}/page/*.xml\")))\n",
    "xml_file_path = glob.glob(f\"{input_folder}/page/*.xml\")[0]\n",
    "\n",
    "print(os.path.normpath(xml_file_path).replace(\"\\\\\", \"/\"))\n",
    "\n",
    "# help(glob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
